<p>8 <strong>experimental plots</strong>, 3 <em>theoretical calculations</em></p>
<p>Overview</p>
<ul>
<li>Conclusion upfront: TBD, for now assume <strong>comparison of TAG, normalized TAG, and exact</strong></li>
</ul>
<p>Experiment setup</p>
<ul>
<li>ESM, classifier on top, level-4 ECs trying to guide towards top 10 [^5]</li>
</ul>
<p>Brief intro to guidance</p>
<ul>
<li><em>Bayes' rule as a specific optimization objective</em> [^4]</li>
<li>Exact vs TAG and why TAG [^7]</li>
</ul>
<!-- - **Dataset proportion of target classes versus model** [^6] -->
<p>Predicting guidance performance from importance reweighting of unconditional samples [^3]</p>
<ul>
<li><strong>exact Gillespie recovers predicted accuracy</strong></li>
<li><strong>TAG severely underperforms</strong></li>
</ul>
<p>Why TAG fails and what to do about it</p>
<ul>
<li><strong>TAG experiences big jumps</strong></li>
<li>Using <strong>single jumps recover forward progress</strong>.</li>
<li><em>due to being unnormalized</em>.</li>
<li><em>Normalization</em>. Briefly discuss projection and truncation. [^2]</li>
</ul>
<p>Single jumps are slow, can we recover speed?</p>
<ul>
<li><strong>Normalized TAG with numerical integration. Normalized TAG with Gillespie.</strong></li>
<li><strong>Comparing normalized predictions with exact.</strong> [^1]</li>
</ul>
<p>How much does this generalize to other problems? Is this specific to the Enzyme problem?</p>
<ul>
<li><strong>Screen other experiments in the paper using the unconditional technique</strong> and compare to TAG results</li>
<li>Use <strong>normalized TAG to see how close we get to ideal</strong> (assume that exact gets the predicted results)</li>
</ul>
<p>[^1] To do this practically, get a subsample of TAG and exact predictions for a set of residues. Plot gradient norm against $p_\text{exact}(y|x_t)-p_\text{TAG}(y|x_t) = [p_\text{exact}(y|\tilde{x}<em>t) - p</em>\text{TAG}(y|\tilde{x}_t)]p(\tilde{x}_t|x_t)$. Note this assumes exact gets a better $p(y|x_t)$ than TAG, which may not always be true.</p>
<p>[^2] Current ESM setup has a mask token to which gradients can accrue, but I just drop it--like conditioning on a transition. $p(y|M, x_t)=p(y|x_t)=\sum_{a\in A} p(y|a, x_t)p(a|x_t)$ so should be able to normalize this total ratio to 1.</p>
<p>[^3] Footnote about conditional probability versus Bayesian update and using update view-point to increase guidance strength.</p>
<p>[^4] To be expanded as a separate post, for now just find out what kind of KL or variational objective this corresponds to. Convert $p(y)$ stuff to expectations, take the log, apply jensens, check online to compare.</p>
<p>[^5] Include paper appendix description as a footnote.</p>
<p>[^6] Removing because that is more related to data guidance not TAG</p>
<p>[^7] TAG eats a lot of memory per backward pass if using model in classifier, whether or not you want that depends on how good the transition estimates are. It's not worth doing that comparison until I think of comparisons to finetuning, including LoRA, preference, and transition matching</p>
