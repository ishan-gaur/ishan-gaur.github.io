We can easily unify all discrete fow matching, discrete diffusion, any-order models, etc. as generative models of a set of variables. In this sense they don't even need to be discrete state, just anything with a finite set of random variables. You can totally generate random variables with continuous ranges, you get need some way to parameterize an appropriate space of density functions to sample from.

Flow matching came from looking for a tractable way to learn continuous normalizing flows, no?

Flow matching and discrete diffusion parameterize a set of sampling distributions given some approximation for the conditional marginals of $p$.

Given a certain sampling algorithm, one can then derive a cost function to select between possible approximations for $p$. For each algorithm, the way the target distribution interacts with the sampling algorithm and the particular one chosen by the user might change the optimal approximate $p$ we want to learn.

We might also approximate $p$ by having the machine learning model learn different quantities related to $p$, like the rates between two states or the actual probability of the next variable given some conditioning variables.

For different sampling algorithms, different kinds of approximation errors will have different impacts. One of the issues is that we normally design the sampling framework based on having the true $p$ in all of our math. So, for example, learning orders becomes a second order problem for flow-matching instead of one of the first design objectives.

But how do you talk about the $p$ the model represents without the sampling algorithm. You only get $p$ as the sampling distribution once you plug it into the sampling algorithm. If you put it into a different algorithm, you'll get a different distribution.

The sampling distribution might change the temperature or the set of marginals that get used. For example, if you do autoregressive sampling with the any-order model, you will get the distribution defined by those marginals instead of a mixture distribution of the joints given by all orders. If, like in flow matching, continuous time approximations lead to sampling multiple positions at once, you are likely to use some of the marginals much more than others.

What's appropriate all depends on what you want to do with the sampling distribution later on.

Since we have finite data, we will always have error. As a result, the question is how to we specify a loss that trades-off the cost of errors with the benefit of getting things right--ie mode collapse to a "real" sample versus covering the set of good solutions but also returning a bunch of junk. That would be much better than temperature. Ideally, we could specify the reward versus cost of mistakes. Then the sampling algorithm could accordingly take decisions.

The problem is now we come to things we can't predict because we're going outside the distribution.

In this sense, it's interesting that the any-order models generate stuff that's so far from the data but do well on data samples. Like data sequences much more quickly drop uncertainty about what's going on.

Maybe this has something to do with ELBO and an objective for sampling rather than just classifying correctly. Like it isn't like the binary classification stuff in the class where we have to just come to a predicted class, and there isn't a probability either way--like the Neyman-Pearson lemma, that in terms of the FPR or whatever, a stochastic decision rule doesn't do better.

Maybe you could have a RNG or something like the brain. The thing is there is always conditioning information--like I'm hungry, saw a billboard earlier, thinking about a book--our generations are always state dependent and evolving, might just be reacting to the last thing that was generated. Could have a model like this that completes random proteins in the context of other random proteins.

So given all this, the question of a foundation model is quite tricky. Really you want a predictor for such a general problem that you can parameterize a set of predictors on top of it algorithmically in such a way that you can easily tune it to get a bunch of high quality behaviors on related tasks with minimal optimization work. In that sense intelligence is different from function approximation in that it needs to be able to automatically understand the new task and loss function and adapt to it.

The question is, what is a possible alternate basis for generative models other than modeling a data distribution? Secondarily, where do the boundaries of equivalence between all these discrete generative models lie?

What is the history of these CTMC-based methods. Continuous time labels for a process where the times don't matter is a bit silly no? OH... This could be useful for parallization, allow the model to pick the times and based on the discretization grid it will get a score. You get orders and parallelization at once!!!